[2021-12-06 15:37:43,509 INFO] Process ID 217601, Process Parent ID 1
[2021-12-06 15:37:43,509 INFO] Torch Version: 1.10.0
[2021-12-06 15:37:44,165 INFO] GPU available: True
[2021-12-06 15:37:44,165 INFO] CuDNN: True
[2021-12-06 15:37:44,165 INFO] Namespace(data_path='./datasets', dataset='dailydialog', max_seq_size=512, learning_rate=0.0001, dropout=0.3, attn_dropout=0.3, mlm_probability=0.0, beta_1=0.9, beta_2=0.9, epsilon=1e-10, l2_reg=0.0, epochs=100, decay=0.8, decay_steps=2, decay_threshold=50, gpu_ranks=[0], gpuid=[], n_gpu=1, report_step=100, valid_steps=300, clip=1.0, batch_size=100, valid_batch_size=100, data_shuffle=True, earlystop=10, inference=False, evalution=False, load_file='checkpoints/bart1_step_16750.pt', result_file='result.txt', max_response_size=30, save_model='checkpoints/bart16', checkpoint_path='./checkpoints/', keep_checkpoint=-1, model_type='bartpsd', param_init=0.1, param_init_glorot=False, pertrain_type='bart', pertrain_path='/data/xkg/projects/pertrain_model/bart-base', is_tune=True, output_attentions=True, return_dict=True, output_hidden_states=True, emb_dim=768, position_encoding=False, model_dtype='fp32', encoder_type='rnn', decoder_type='style', layers=-1, enc_layers=2, dec_layers=2, model_dim=768, enc_dim=500, dec_dim=500, context_gate=None, global_attention='general', global_attention_function='softmax', self_attn_type='scaled-dot', max_relative_positions=0, heads=8, transformer_ff=2048, aan_useffn=False, copy_attn=False, copy_attn_type=None, generator_function='softmax', copy_attn_force=False, reuse_copy_attn=False, copy_loss_by_seqlength=False, coverage_attn=False, lambda_coverage=0.0, loss_scale=0, class_num=10)
[2021-12-06 15:37:44,295 INFO] Building train batcher ...
[2021-12-06 15:37:46,958 INFO] Building valid batcher ...
[2021-12-06 15:37:47,153 INFO] Building model ...
[2021-12-06 15:37:50,318 INFO] Initializing parameters ...
[2021-12-06 15:37:59,081 INFO] 
BartPSD(
  (decoder): PersonalStyle(
    (style_emb): StyleEmbeddings(
      (emb): Embedding(10, 768)
      (dropout): Dropout(p=0.5, inplace=False)
      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (dropout): Dropout(p=0.3, inplace=False)
    (sigmoid): Sigmoid()
  )
  (emb2model): Linear(in_features=768, out_features=768, bias=True)
  (bart): MyBart(
    (bart): BartModel(
      (shared): Embedding(50266, 768)
      (encoder): BartEncoder(
        (embed_tokens): Embedding(50266, 768)
        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
        (layers): ModuleList(
          (0): BartEncoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): BartEncoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): BartEncoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): BartEncoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): BartEncoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): BartEncoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (decoder): BartDecoder(
        (embed_tokens): Embedding(50266, 768)
        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
        (layers): ModuleList(
          (0): BartDecoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): BartDecoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): BartDecoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): BartDecoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): BartDecoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): BartDecoderLayer(
            (self_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): BartAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=768, out_features=50266, bias=True)
    (1): Cast()
    (2): LogSoftmax(dim=-1)
  )
  (classifier): Sequential(
    (0): Linear(in_features=10, out_features=10, bias=True)
    (1): Cast()
    (2): LogSoftmax(dim=-1)
  )
)
[2021-12-06 15:37:59,082 INFO] *** [ The number of the model's parameters ] ***
[2021-12-06 15:37:59,082 INFO] embeddings: 3072
[2021-12-06 15:37:59,082 INFO] encoder: 57498624
[2021-12-06 15:37:59,082 INFO] decoder: 43324416
[2021-12-06 15:37:59,082 INFO] model: 139480666
[2021-12-06 15:37:59,083 INFO] Building trianer ...
[2021-12-06 15:37:59,083 INFO] Start to train...
[2021-12-06 15:37:59,084 INFO] train set: 669
[2021-12-06 15:37:59,084 INFO] validate set: 63
[2021-12-06 15:37:59,084 INFO] *** [ Epoch 1 / 100 ] ***
[2021-12-06 15:38:27,337 INFO] step:    100 /  66900, during_time: 28.25, avg_loss: 56.512, accuracy: 17.476, bi_accuracy: 23.180, lr: 0.0001000
[2021-12-06 15:38:54,403 INFO] step:    200 /  66900, during_time: 55.32, avg_loss: 48.794, accuracy: 21.647, bi_accuracy: 28.200, lr: 0.0001000
[2021-12-06 15:39:21,532 INFO] step:    300 /  66900, during_time: 82.45, avg_loss: 43.953, accuracy: 24.118, bi_accuracy: 31.640, lr: 0.0001000
[2021-12-06 15:39:48,755 INFO] step:    400 /  66900, during_time: 109.67, avg_loss: 40.655, accuracy: 25.695, bi_accuracy: 34.018, lr: 0.0001000
[2021-12-06 15:40:16,054 INFO] step:    500 /  66900, during_time: 136.97, avg_loss: 38.191, accuracy: 26.787, bi_accuracy: 35.780, lr: 0.0001000
[2021-12-06 15:40:42,473 INFO] step:    600 /  66900, during_time: 163.39, avg_loss: 36.153, accuracy: 27.628, bi_accuracy: 37.248, lr: 0.0001000
[2021-12-06 15:41:01,098 INFO] step:    669 /  66900, during_time: 182.01, avg_loss: 34.843, accuracy: 28.119, bi_accuracy: 38.284, lr: 0.0001000
[2021-12-06 15:41:01,098 INFO] *** [ Epoch 2 / 100 ] ***
[2021-12-06 15:41:27,954 INFO] step:    769 /  66900, during_time: 26.86, avg_loss: 21.928, accuracy: 32.635, bi_accuracy: 47.290, lr: 0.0001000
[2021-12-06 15:41:55,091 INFO] step:    869 /  66900, during_time: 53.99, avg_loss: 20.744, accuracy: 32.935, bi_accuracy: 48.545, lr: 0.0001000
[2021-12-06 15:42:22,283 INFO] step:    969 /  66900, during_time: 81.18, avg_loss: 19.833, accuracy: 33.219, bi_accuracy: 49.400, lr: 0.0001000
[2021-12-06 15:42:22,284 INFO] Validating ...
[2021-12-06 15:42:31,028 INFO] Validation example number: 6255
[2021-12-06 15:42:31,028 INFO] Validation time: 8.74
[2021-12-06 15:42:31,028 INFO] Validation average loss: 3.792
[2021-12-06 15:42:31,028 INFO] Validation perplexity: 44.335
[2021-12-06 15:42:31,028 INFO] Validation accuracy: 37.039
[2021-12-06 15:42:31,028 INFO] Validation bi_accuracy: 64.444
[2021-12-06 15:42:31,028 INFO] Model is improving acc: 0.000 --> 37.039
[2021-12-06 15:42:31,028 INFO] Model is improving ppl: inf --> 44.335
[2021-12-06 15:42:31,312 INFO] Saving checkpoint checkpoints/bart16_step_969.pt
[2021-12-06 15:43:01,932 INFO] step:   1069 /  66900, during_time: 120.83, avg_loss: 16.253, accuracy: 34.186, bi_accuracy: 52.676, lr: 0.0001000
[2021-12-06 15:43:28,512 INFO] step:   1169 /  66900, during_time: 147.41, avg_loss: 13.929, accuracy: 34.868, bi_accuracy: 55.662, lr: 0.0001000
[2021-12-06 15:43:54,223 INFO] step:   1269 /  66900, during_time: 173.12, avg_loss: 12.355, accuracy: 35.380, bi_accuracy: 58.045, lr: 0.0001000
[2021-12-06 15:43:54,223 INFO] Validating ...
[2021-12-06 15:44:02,955 INFO] Validation example number: 6255
[2021-12-06 15:44:02,956 INFO] Validation time: 8.73
[2021-12-06 15:44:02,956 INFO] Validation average loss: 3.634
[2021-12-06 15:44:02,956 INFO] Validation perplexity: 37.867
[2021-12-06 15:44:02,956 INFO] Validation accuracy: 38.072
[2021-12-06 15:44:02,956 INFO] Validation bi_accuracy: 63.789
[2021-12-06 15:44:02,956 INFO] Model is improving acc: 37.039 --> 38.072
[2021-12-06 15:44:02,956 INFO] Model is improving ppl: 44.335 --> 37.867
[2021-12-06 15:44:03,221 INFO] Saving checkpoint checkpoints/bart16_step_1269.pt
[2021-12-06 15:44:25,104 INFO] step:   1338 /  66900, during_time: 204.01, avg_loss: 11.531, accuracy: 35.694, bi_accuracy: 59.404, lr: 0.0001000
[2021-12-06 15:44:25,105 INFO] Validating ...
[2021-12-06 15:44:33,828 INFO] Validation example number: 6255
[2021-12-06 15:44:33,828 INFO] Validation time: 8.72
[2021-12-06 15:44:33,828 INFO] Validation average loss: 3.604
[2021-12-06 15:44:33,828 INFO] Validation perplexity: 36.754
[2021-12-06 15:44:33,828 INFO] Validation accuracy: 38.224
[2021-12-06 15:44:33,828 INFO] Validation bi_accuracy: 59.648
[2021-12-06 15:44:33,829 INFO] Model is improving acc: 38.072 --> 38.224
[2021-12-06 15:44:33,829 INFO] Model is improving ppl: 37.867 --> 36.754
[2021-12-06 15:44:34,073 INFO] Saving checkpoint checkpoints/bart16_step_1338.pt
[2021-12-06 15:44:38,750 INFO] *** [ Epoch 3 / 100 ] ***
[2021-12-06 15:45:05,524 INFO] step:   1438 /  66900, during_time: 26.77, avg_loss: 17.228, accuracy: 33.736, bi_accuracy: 49.620, lr: 0.0000800
[2021-12-06 15:45:32,694 INFO] step:   1538 /  66900, during_time: 53.94, avg_loss: 15.500, accuracy: 34.398, bi_accuracy: 53.970, lr: 0.0000800
[2021-12-06 15:45:59,818 INFO] step:   1638 /  66900, during_time: 81.07, avg_loss: 14.414, accuracy: 34.779, bi_accuracy: 56.373, lr: 0.0000800
[2021-12-06 15:45:59,818 INFO] Validating ...
[2021-12-06 15:46:08,544 INFO] Validation example number: 6255
[2021-12-06 15:46:08,544 INFO] Validation time: 8.73
[2021-12-06 15:46:08,545 INFO] Validation average loss: 3.551
[2021-12-06 15:46:08,545 INFO] Validation perplexity: 34.836
[2021-12-06 15:46:08,545 INFO] Validation accuracy: 38.688
[2021-12-06 15:46:08,545 INFO] Validation bi_accuracy: 67.130
[2021-12-06 15:46:08,545 INFO] Model is improving acc: 38.224 --> 38.688
[2021-12-06 15:46:08,545 INFO] Model is improving ppl: 36.754 --> 34.836
[2021-12-06 15:46:08,793 INFO] Saving checkpoint checkpoints/bart16_step_1638.pt
[2021-12-06 15:46:39,790 INFO] step:   1738 /  66900, during_time: 121.04, avg_loss: 11.918, accuracy: 36.265, bi_accuracy: 61.041, lr: 0.0000800
[2021-12-06 15:47:06,404 INFO] step:   1838 /  66900, during_time: 147.65, avg_loss: 10.292, accuracy: 37.313, bi_accuracy: 64.838, lr: 0.0000800
[2021-12-06 15:47:32,082 INFO] step:   1938 /  66900, during_time: 173.33, avg_loss: 9.189, accuracy: 38.110, bi_accuracy: 67.755, lr: 0.0000800
[2021-12-06 15:47:32,082 INFO] Validating ...
[2021-12-06 15:47:40,834 INFO] Validation example number: 6255
[2021-12-06 15:47:40,834 INFO] Validation time: 8.75
[2021-12-06 15:47:40,834 INFO] Validation average loss: 3.494
[2021-12-06 15:47:40,834 INFO] Validation perplexity: 32.904
[2021-12-06 15:47:40,834 INFO] Validation accuracy: 39.422
[2021-12-06 15:47:40,834 INFO] Validation bi_accuracy: 63.613
[2021-12-06 15:47:40,834 INFO] Model is improving acc: 38.688 --> 39.422
[2021-12-06 15:47:40,834 INFO] Model is improving ppl: 34.836 --> 32.904
[2021-12-06 15:47:41,084 INFO] Saving checkpoint checkpoints/bart16_step_1938.pt
[2021-12-06 15:48:03,387 INFO] step:   2007 /  66900, during_time: 204.64, avg_loss: 8.611, accuracy: 38.575, bi_accuracy: 69.345, lr: 0.0000800
[2021-12-06 15:48:03,388 INFO] Validating ...
[2021-12-06 15:48:12,132 INFO] Validation example number: 6255
[2021-12-06 15:48:12,132 INFO] Validation time: 8.74
[2021-12-06 15:48:12,132 INFO] Validation average loss: 3.494
[2021-12-06 15:48:12,132 INFO] Validation perplexity: 32.912
[2021-12-06 15:48:12,132 INFO] Validation accuracy: 39.330
[2021-12-06 15:48:12,132 INFO] Validation bi_accuracy: 64.700
[2021-12-06 15:48:12,132 INFO] Stalled patience: 1 / 10
[2021-12-06 15:48:12,132 INFO] *** [ Epoch 4 / 100 ] ***
[2021-12-06 15:48:38,923 INFO] step:   2107 /  66900, during_time: 26.79, avg_loss: 12.599, accuracy: 34.753, bi_accuracy: 58.020, lr: 0.0000800
[2021-12-06 15:49:06,016 INFO] step:   2207 /  66900, during_time: 53.88, avg_loss: 11.520, accuracy: 35.506, bi_accuracy: 61.325, lr: 0.0000800
[2021-12-06 15:49:33,164 INFO] step:   2307 /  66900, during_time: 81.03, avg_loss: 10.824, accuracy: 35.914, bi_accuracy: 63.147, lr: 0.0000800
[2021-12-06 15:49:33,164 INFO] Validating ...
[2021-12-06 15:49:41,898 INFO] Validation example number: 6255
[2021-12-06 15:49:41,899 INFO] Validation time: 8.73
[2021-12-06 15:49:41,899 INFO] Validation average loss: 3.438
[2021-12-06 15:49:41,899 INFO] Validation perplexity: 31.136
[2021-12-06 15:49:41,899 INFO] Validation accuracy: 39.909
[2021-12-06 15:49:41,899 INFO] Validation bi_accuracy: 69.033
[2021-12-06 15:49:41,899 INFO] Model is improving acc: 39.422 --> 39.909
[2021-12-06 15:49:41,899 INFO] Model is improving ppl: 32.904 --> 31.136
[2021-12-06 15:49:42,159 INFO] Saving checkpoint checkpoints/bart16_step_2307.pt
[2021-12-06 15:50:12,806 INFO] step:   2407 /  66900, during_time: 120.67, avg_loss: 9.049, accuracy: 37.959, bi_accuracy: 68.089, lr: 0.0000800
[2021-12-06 15:50:39,435 INFO] step:   2507 /  66900, during_time: 147.30, avg_loss: 7.891, accuracy: 39.423, bi_accuracy: 71.745, lr: 0.0000800
[2021-12-06 15:51:05,134 INFO] step:   2607 /  66900, during_time: 173.00, avg_loss: 7.100, accuracy: 40.529, bi_accuracy: 74.413, lr: 0.0000800
[2021-12-06 15:51:05,134 INFO] Validating ...
[2021-12-06 15:51:13,875 INFO] Validation example number: 6255
[2021-12-06 15:51:13,875 INFO] Validation time: 8.74
[2021-12-06 15:51:13,875 INFO] Validation average loss: 3.459
[2021-12-06 15:51:13,875 INFO] Validation perplexity: 31.791
[2021-12-06 15:51:13,875 INFO] Validation accuracy: 39.716
[2021-12-06 15:51:13,875 INFO] Validation bi_accuracy: 66.011
[2021-12-06 15:51:13,876 INFO] Stalled patience: 1 / 10
[2021-12-06 15:51:31,780 INFO] step:   2676 /  66900, during_time: 199.65, avg_loss: 6.683, accuracy: 41.211, bi_accuracy: 75.866, lr: 0.0000800
[2021-12-06 15:51:31,780 INFO] Validating ...
[2021-12-06 15:51:40,531 INFO] Validation example number: 6255
[2021-12-06 15:51:40,532 INFO] Validation time: 8.75
[2021-12-06 15:51:40,533 INFO] Validation average loss: 3.477
[2021-12-06 15:51:40,533 INFO] Validation perplexity: 32.360
[2021-12-06 15:51:40,533 INFO] Validation accuracy: 39.906
[2021-12-06 15:51:40,533 INFO] Validation bi_accuracy: 68.329
[2021-12-06 15:51:40,533 INFO] Stalled patience: 2 / 10
[2021-12-06 15:51:40,533 INFO] *** [ Epoch 5 / 100 ] ***
[2021-12-06 15:52:07,432 INFO] step:   2776 /  66900, during_time: 26.90, avg_loss: 10.038, accuracy: 35.619, bi_accuracy: 63.170, lr: 0.0000640
[2021-12-06 15:52:34,578 INFO] step:   2876 /  66900, during_time: 54.04, avg_loss: 9.097, accuracy: 36.361, bi_accuracy: 66.610, lr: 0.0000640
[2021-12-06 15:53:01,758 INFO] step:   2976 /  66900, during_time: 81.22, avg_loss: 8.587, accuracy: 36.742, bi_accuracy: 68.217, lr: 0.0000640
[2021-12-06 15:53:01,758 INFO] Validating ...
[2021-12-06 15:53:10,488 INFO] Validation example number: 6255
[2021-12-06 15:53:10,488 INFO] Validation time: 8.73
[2021-12-06 15:53:10,488 INFO] Validation average loss: 3.361
[2021-12-06 15:53:10,489 INFO] Validation perplexity: 28.806
[2021-12-06 15:53:10,489 INFO] Validation accuracy: 40.911
[2021-12-06 15:53:10,489 INFO] Validation bi_accuracy: 68.137
[2021-12-06 15:53:10,489 INFO] Model is improving acc: 39.909 --> 40.911
[2021-12-06 15:53:10,489 INFO] Model is improving ppl: 31.136 --> 28.806
[2021-12-06 15:53:10,737 INFO] Saving checkpoint checkpoints/bart16_step_2976.pt
[2021-12-06 15:53:41,263 INFO] step:   3076 /  66900, during_time: 120.73, avg_loss: 7.225, accuracy: 39.722, bi_accuracy: 73.147, lr: 0.0000640
[2021-12-06 15:54:07,876 INFO] step:   3176 /  66900, during_time: 147.34, avg_loss: 6.341, accuracy: 41.816, bi_accuracy: 76.606, lr: 0.0000640
[2021-12-06 15:54:33,471 INFO] step:   3276 /  66900, during_time: 172.94, avg_loss: 5.732, accuracy: 43.401, bi_accuracy: 79.025, lr: 0.0000640
[2021-12-06 15:54:33,471 INFO] Validating ...
[2021-12-06 15:54:42,211 INFO] Validation example number: 6255
[2021-12-06 15:54:42,211 INFO] Validation time: 8.74
[2021-12-06 15:54:42,211 INFO] Validation average loss: 3.486
[2021-12-06 15:54:42,211 INFO] Validation perplexity: 32.653
[2021-12-06 15:54:42,211 INFO] Validation accuracy: 40.578
[2021-12-06 15:54:42,211 INFO] Validation bi_accuracy: 68.281
[2021-12-06 15:54:42,211 INFO] Stalled patience: 1 / 10
[2021-12-06 15:55:00,106 INFO] step:   3345 /  66900, during_time: 199.57, avg_loss: 5.408, accuracy: 44.373, bi_accuracy: 80.271, lr: 0.0000640
[2021-12-06 15:55:00,106 INFO] Validating ...
[2021-12-06 15:55:08,833 INFO] Validation example number: 6255
[2021-12-06 15:55:08,833 INFO] Validation time: 8.73
[2021-12-06 15:55:08,833 INFO] Validation average loss: 3.528
[2021-12-06 15:55:08,833 INFO] Validation perplexity: 34.071
[2021-12-06 15:55:08,833 INFO] Validation accuracy: 40.335
[2021-12-06 15:55:08,833 INFO] Validation bi_accuracy: 69.800
[2021-12-06 15:55:08,833 INFO] Stalled patience: 2 / 10
[2021-12-06 15:55:08,833 INFO] *** [ Epoch 6 / 100 ] ***
[2021-12-06 15:55:35,670 INFO] step:   3445 /  66900, during_time: 26.84, avg_loss: 8.221, accuracy: 35.946, bi_accuracy: 67.970, lr: 0.0000640
[2021-12-06 15:56:02,859 INFO] step:   3545 /  66900, during_time: 54.03, avg_loss: 7.458, accuracy: 36.985, bi_accuracy: 70.925, lr: 0.0000640
[2021-12-06 15:56:29,962 INFO] step:   3645 /  66900, during_time: 81.13, avg_loss: 7.107, accuracy: 37.455, bi_accuracy: 72.397, lr: 0.0000640
[2021-12-06 15:56:29,963 INFO] Validating ...
[2021-12-06 15:56:38,707 INFO] Validation example number: 6255
[2021-12-06 15:56:38,707 INFO] Validation time: 8.74
[2021-12-06 15:56:38,707 INFO] Validation average loss: 3.332
[2021-12-06 15:56:38,707 INFO] Validation perplexity: 27.990
[2021-12-06 15:56:38,707 INFO] Validation accuracy: 41.384
[2021-12-06 15:56:38,707 INFO] Validation bi_accuracy: 68.329
[2021-12-06 15:56:38,707 INFO] Model is improving acc: 40.911 --> 41.384
[2021-12-06 15:56:38,708 INFO] Model is improving ppl: 28.806 --> 27.990
[2021-12-06 15:56:38,953 INFO] Saving checkpoint checkpoints/bart16_step_3645.pt
[2021-12-06 15:57:09,508 INFO] step:   3745 /  66900, during_time: 120.67, avg_loss: 6.026, accuracy: 41.264, bi_accuracy: 76.885, lr: 0.0000640
[2021-12-06 15:57:36,184 INFO] step:   3845 /  66900, during_time: 147.35, avg_loss: 5.313, accuracy: 43.943, bi_accuracy: 79.988, lr: 0.0000640
[2021-12-06 15:58:01,865 INFO] step:   3945 /  66900, during_time: 173.03, avg_loss: 4.827, accuracy: 45.907, bi_accuracy: 81.981, lr: 0.0000640
[2021-12-06 15:58:01,866 INFO] Validating ...
[2021-12-06 15:58:10,618 INFO] Validation example number: 6255
[2021-12-06 15:58:10,619 INFO] Validation time: 8.75
[2021-12-06 15:58:10,619 INFO] Validation average loss: 3.518
[2021-12-06 15:58:10,619 INFO] Validation perplexity: 33.722
[2021-12-06 15:58:10,619 INFO] Validation accuracy: 41.018
[2021-12-06 15:58:10,619 INFO] Validation bi_accuracy: 68.889
[2021-12-06 15:58:10,619 INFO] Stalled patience: 1 / 10
[2021-12-06 15:58:28,550 INFO] step:   4014 /  66900, during_time: 199.72, avg_loss: 4.565, accuracy: 47.083, bi_accuracy: 83.069, lr: 0.0000640
[2021-12-06 15:58:28,550 INFO] Validating ...
[2021-12-06 15:58:37,482 INFO] Validation example number: 6255
[2021-12-06 15:58:37,482 INFO] Validation time: 8.93
[2021-12-06 15:58:37,482 INFO] Validation average loss: 3.578
[2021-12-06 15:58:37,482 INFO] Validation perplexity: 35.803
[2021-12-06 15:58:37,482 INFO] Validation accuracy: 40.429
[2021-12-06 15:58:37,482 INFO] Validation bi_accuracy: 68.473
[2021-12-06 15:58:37,482 INFO] Stalled patience: 2 / 10
[2021-12-06 15:58:37,482 INFO] *** [ Epoch 7 / 100 ] ***
[2021-12-06 15:59:05,061 INFO] step:   4114 /  66900, during_time: 27.58, avg_loss: 7.211, accuracy: 36.348, bi_accuracy: 69.910, lr: 0.0000512
[2021-12-06 15:59:32,484 INFO] step:   4214 /  66900, during_time: 55.00, avg_loss: 6.651, accuracy: 37.515, bi_accuracy: 72.725, lr: 0.0000512
[2021-12-06 15:59:59,610 INFO] step:   4314 /  66900, during_time: 82.13, avg_loss: 6.350, accuracy: 38.073, bi_accuracy: 74.397, lr: 0.0000512
[2021-12-06 15:59:59,610 INFO] Validating ...
[2021-12-06 16:00:08,325 INFO] Validation example number: 6255
[2021-12-06 16:00:08,326 INFO] Validation time: 8.72
[2021-12-06 16:00:08,326 INFO] Validation average loss: 3.311
[2021-12-06 16:00:08,326 INFO] Validation perplexity: 27.418
[2021-12-06 16:00:08,326 INFO] Validation accuracy: 41.873
[2021-12-06 16:00:08,326 INFO] Validation bi_accuracy: 68.489
[2021-12-06 16:00:08,326 INFO] Model is improving acc: 41.384 --> 41.873
[2021-12-06 16:00:08,326 INFO] Model is improving ppl: 27.990 --> 27.418
[2021-12-06 16:00:08,570 INFO] Saving checkpoint checkpoints/bart16_step_4314.pt
[2021-12-06 16:00:38,967 INFO] step:   4414 /  66900, during_time: 121.48, avg_loss: 5.362, accuracy: 42.865, bi_accuracy: 78.734, lr: 0.0000512
[2021-12-06 16:01:05,563 INFO] step:   4514 /  66900, during_time: 148.08, avg_loss: 4.716, accuracy: 46.227, bi_accuracy: 81.723, lr: 0.0000512
[2021-12-06 16:01:31,196 INFO] step:   4614 /  66900, during_time: 173.71, avg_loss: 4.272, accuracy: 48.630, bi_accuracy: 83.747, lr: 0.0000512
[2021-12-06 16:01:31,196 INFO] Validating ...
[2021-12-06 16:01:39,942 INFO] Validation example number: 6255
[2021-12-06 16:01:39,942 INFO] Validation time: 8.75
[2021-12-06 16:01:39,942 INFO] Validation average loss: 3.579
[2021-12-06 16:01:39,942 INFO] Validation perplexity: 35.850
[2021-12-06 16:01:39,942 INFO] Validation accuracy: 41.343
[2021-12-06 16:01:39,942 INFO] Validation bi_accuracy: 68.873
[2021-12-06 16:01:39,942 INFO] Stalled patience: 1 / 10
[2021-12-06 16:01:57,822 INFO] step:   4683 /  66900, during_time: 200.34, avg_loss: 4.032, accuracy: 50.101, bi_accuracy: 84.744, lr: 0.0000512
[2021-12-06 16:01:57,822 INFO] Validating ...
[2021-12-06 16:02:06,559 INFO] Validation example number: 6255
[2021-12-06 16:02:06,559 INFO] Validation time: 8.74
[2021-12-06 16:02:06,559 INFO] Validation average loss: 3.682
[2021-12-06 16:02:06,559 INFO] Validation perplexity: 39.716
[2021-12-06 16:02:06,559 INFO] Validation accuracy: 40.447
[2021-12-06 16:02:06,559 INFO] Validation bi_accuracy: 69.464
[2021-12-06 16:02:06,559 INFO] Stalled patience: 2 / 10
[2021-12-06 16:02:06,559 INFO] *** [ Epoch 8 / 100 ] ***
[2021-12-06 16:02:33,341 INFO] step:   4783 /  66900, during_time: 26.78, avg_loss: 6.282, accuracy: 36.857, bi_accuracy: 73.890, lr: 0.0000512
[2021-12-06 16:03:00,375 INFO] step:   4883 /  66900, during_time: 53.82, avg_loss: 5.814, accuracy: 38.111, bi_accuracy: 76.365, lr: 0.0000512
[2021-12-06 16:03:27,446 INFO] step:   4983 /  66900, during_time: 80.89, avg_loss: 5.609, accuracy: 38.679, bi_accuracy: 77.503, lr: 0.0000512
[2021-12-06 16:03:27,446 INFO] Validating ...
[2021-12-06 16:03:36,116 INFO] Validation example number: 6255
[2021-12-06 16:03:36,117 INFO] Validation time: 8.67
[2021-12-06 16:03:36,117 INFO] Validation average loss: 3.304
[2021-12-06 16:03:36,117 INFO] Validation perplexity: 27.209
[2021-12-06 16:03:36,117 INFO] Validation accuracy: 42.198
[2021-12-06 16:03:36,117 INFO] Validation bi_accuracy: 70.120
[2021-12-06 16:03:36,117 INFO] Model is improving acc: 41.873 --> 42.198
[2021-12-06 16:03:36,117 INFO] Model is improving ppl: 27.418 --> 27.209
[2021-12-06 16:03:36,368 INFO] Saving checkpoint checkpoints/bart16_step_4983.pt
[2021-12-06 16:04:07,002 INFO] step:   5083 /  66900, during_time: 120.44, avg_loss: 4.730, accuracy: 44.251, bi_accuracy: 81.418, lr: 0.0000512
[2021-12-06 16:04:33,659 INFO] step:   5183 /  66900, during_time: 147.10, avg_loss: 4.164, accuracy: 48.119, bi_accuracy: 84.008, lr: 0.0000512
[2021-12-06 16:04:59,187 INFO] step:   5283 /  66900, during_time: 172.63, avg_loss: 3.778, accuracy: 50.866, bi_accuracy: 85.666, lr: 0.0000512
[2021-12-06 16:04:59,187 INFO] Validating ...
[2021-12-06 16:05:07,864 INFO] Validation example number: 6255
[2021-12-06 16:05:07,865 INFO] Validation time: 8.68
[2021-12-06 16:05:07,865 INFO] Validation average loss: 3.657
[2021-12-06 16:05:07,865 INFO] Validation perplexity: 38.729
[2021-12-06 16:05:07,865 INFO] Validation accuracy: 41.226
[2021-12-06 16:05:07,865 INFO] Validation bi_accuracy: 70.488
[2021-12-06 16:05:07,865 INFO] Stalled patience: 1 / 10
[2021-12-06 16:05:25,610 INFO] step:   5352 /  66900, during_time: 199.05, avg_loss: 3.564, accuracy: 52.536, bi_accuracy: 86.537, lr: 0.0000512
[2021-12-06 16:05:25,610 INFO] Validating ...
[2021-12-06 16:05:34,258 INFO] Validation example number: 6255
[2021-12-06 16:05:34,258 INFO] Validation time: 8.65
[2021-12-06 16:05:34,258 INFO] Validation average loss: 3.753
[2021-12-06 16:05:34,258 INFO] Validation perplexity: 42.639
[2021-12-06 16:05:34,258 INFO] Validation accuracy: 40.194
[2021-12-06 16:05:34,258 INFO] Validation bi_accuracy: 69.321
[2021-12-06 16:05:34,258 INFO] Stalled patience: 2 / 10
[2021-12-06 16:05:34,258 INFO] *** [ Epoch 9 / 100 ] ***
[2021-12-06 16:06:00,859 INFO] step:   5452 /  66900, during_time: 26.60, avg_loss: 5.896, accuracy: 37.145, bi_accuracy: 75.720, lr: 0.0000410
[2021-12-06 16:06:27,787 INFO] step:   5552 /  66900, during_time: 53.53, avg_loss: 5.463, accuracy: 38.506, bi_accuracy: 78.040, lr: 0.0000410
[2021-12-06 16:06:54,700 INFO] step:   5652 /  66900, during_time: 80.44, avg_loss: 5.218, accuracy: 39.133, bi_accuracy: 79.300, lr: 0.0000410
[2021-12-06 16:06:54,700 INFO] Validating ...
[2021-12-06 16:07:03,351 INFO] Validation example number: 6255
[2021-12-06 16:07:03,351 INFO] Validation time: 8.65
[2021-12-06 16:07:03,351 INFO] Validation average loss: 3.297
[2021-12-06 16:07:03,351 INFO] Validation perplexity: 27.041
[2021-12-06 16:07:03,352 INFO] Validation accuracy: 42.580
[2021-12-06 16:07:03,352 INFO] Validation bi_accuracy: 70.280
[2021-12-06 16:07:03,352 INFO] Model is improving acc: 42.198 --> 42.580
[2021-12-06 16:07:03,352 INFO] Model is improving ppl: 27.209 --> 27.041
[2021-12-06 16:07:03,599 INFO] Saving checkpoint checkpoints/bart16_step_5652.pt
[2021-12-06 16:07:33,980 INFO] step:   5752 /  66900, during_time: 119.72, avg_loss: 4.380, accuracy: 45.523, bi_accuracy: 82.978, lr: 0.0000410
[2021-12-06 16:08:00,359 INFO] step:   5852 /  66900, during_time: 146.10, avg_loss: 3.838, accuracy: 49.946, bi_accuracy: 85.357, lr: 0.0000410
[2021-12-06 16:08:25,812 INFO] step:   5952 /  66900, during_time: 171.55, avg_loss: 3.466, accuracy: 53.101, bi_accuracy: 86.911, lr: 0.0000410
[2021-12-06 16:08:25,812 INFO] Validating ...
[2021-12-06 16:08:34,462 INFO] Validation example number: 6255
[2021-12-06 16:08:34,463 INFO] Validation time: 8.65
[2021-12-06 16:08:34,463 INFO] Validation average loss: 3.747
[2021-12-06 16:08:34,463 INFO] Validation perplexity: 42.404
[2021-12-06 16:08:34,463 INFO] Validation accuracy: 41.324
[2021-12-06 16:08:34,463 INFO] Validation bi_accuracy: 70.136
[2021-12-06 16:08:34,463 INFO] Stalled patience: 1 / 10
[2021-12-06 16:08:52,176 INFO] step:   6021 /  66900, during_time: 197.92, avg_loss: 3.263, accuracy: 54.941, bi_accuracy: 87.706, lr: 0.0000410
[2021-12-06 16:08:52,176 INFO] Validating ...
[2021-12-06 16:09:00,811 INFO] Validation example number: 6255
[2021-12-06 16:09:00,811 INFO] Validation time: 8.63
[2021-12-06 16:09:00,811 INFO] Validation average loss: 3.753
[2021-12-06 16:09:00,811 INFO] Validation perplexity: 42.653
[2021-12-06 16:09:00,811 INFO] Validation accuracy: 41.362
[2021-12-06 16:09:00,811 INFO] Validation bi_accuracy: 68.601
[2021-12-06 16:09:00,811 INFO] Stalled patience: 2 / 10
[2021-12-06 16:09:00,811 INFO] *** [ Epoch 10 / 100 ] ***
[2021-12-06 16:09:27,408 INFO] step:   6121 /  66900, during_time: 26.60, avg_loss: 5.306, accuracy: 37.514, bi_accuracy: 79.090, lr: 0.0000410
[2021-12-06 16:09:54,314 INFO] step:   6221 /  66900, during_time: 53.50, avg_loss: 4.990, accuracy: 39.008, bi_accuracy: 80.555, lr: 0.0000410
[2021-12-06 16:10:21,234 INFO] step:   6321 /  66900, during_time: 80.42, avg_loss: 4.804, accuracy: 39.663, bi_accuracy: 81.510, lr: 0.0000410
[2021-12-06 16:10:21,234 INFO] Validating ...
[2021-12-06 16:10:29,879 INFO] Validation example number: 6255
[2021-12-06 16:10:29,879 INFO] Validation time: 8.64
[2021-12-06 16:10:29,879 INFO] Validation average loss: 3.293
[2021-12-06 16:10:29,879 INFO] Validation perplexity: 26.911
[2021-12-06 16:10:29,879 INFO] Validation accuracy: 42.843
[2021-12-06 16:10:29,879 INFO] Validation bi_accuracy: 70.024
[2021-12-06 16:10:29,879 INFO] Model is improving acc: 42.580 --> 42.843
[2021-12-06 16:10:29,879 INFO] Model is improving ppl: 27.041 --> 26.911
[2021-12-06 16:10:30,127 INFO] Saving checkpoint checkpoints/bart16_step_6321.pt
[2021-12-06 16:11:00,981 INFO] step:   6421 /  66900, during_time: 120.17, avg_loss: 4.026, accuracy: 46.702, bi_accuracy: 84.692, lr: 0.0000410
[2021-12-06 16:11:27,511 INFO] step:   6521 /  66900, during_time: 146.70, avg_loss: 3.523, accuracy: 51.498, bi_accuracy: 86.761, lr: 0.0000410
[2021-12-06 16:11:53,055 INFO] step:   6621 /  66900, during_time: 172.24, avg_loss: 3.179, accuracy: 54.887, bi_accuracy: 88.115, lr: 0.0000410
[2021-12-06 16:11:53,056 INFO] Validating ...
[2021-12-06 16:12:02,064 INFO] Validation example number: 6255
[2021-12-06 16:12:02,064 INFO] Validation time: 9.01
[2021-12-06 16:12:02,064 INFO] Validation average loss: 3.780
[2021-12-06 16:12:02,064 INFO] Validation perplexity: 43.836
[2021-12-06 16:12:02,064 INFO] Validation accuracy: 41.468
[2021-12-06 16:12:02,064 INFO] Validation bi_accuracy: 69.097
[2021-12-06 16:12:02,064 INFO] Stalled patience: 1 / 10
[2021-12-06 16:12:20,287 INFO] step:   6690 /  66900, during_time: 199.48, avg_loss: 2.989, accuracy: 56.863, bi_accuracy: 88.796, lr: 0.0000410
[2021-12-06 16:12:20,287 INFO] Validating ...
[2021-12-06 16:12:29,131 INFO] Validation example number: 6255
[2021-12-06 16:12:29,131 INFO] Validation time: 8.84
[2021-12-06 16:12:29,131 INFO] Validation average loss: 3.822
[2021-12-06 16:12:29,131 INFO] Validation perplexity: 45.675
[2021-12-06 16:12:29,131 INFO] Validation accuracy: 41.288
[2021-12-06 16:12:29,131 INFO] Validation bi_accuracy: 70.104
[2021-12-06 16:12:29,131 INFO] Stalled patience: 2 / 10
[2021-12-06 16:12:29,131 INFO] *** [ Epoch 11 / 100 ] ***
[2021-12-06 16:12:55,849 INFO] step:   6790 /  66900, during_time: 26.72, avg_loss: 5.132, accuracy: 37.964, bi_accuracy: 79.480, lr: 0.0000328
