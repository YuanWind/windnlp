[Trainer]
# The output directory where the model predictions and checkpoints will be written.
output_dir = ${Data:global_dir}/outs/${Save:postfix}
resume_from_checkpoint= None
num_train_epochs= 50
# -1代表关闭，0代表连续 4 轮没提升就停止训练，>0的数字代表具体的哪一轮停止
early_stop_mode = -1
do_train= true
do_eval= true
do_predict= true
# None, steps, epoch
evaluation_strategy= epoch
eval_steps= 10
learning_rate= 3e-4
per_device_train_batch_size= 32
gradient_accumulation_steps= 1
per_device_eval_batch_size= 32
eval_accumulation_steps= 1
seed= 42
metric_for_best_model= ppl
# 决定 metric_for_best_model 是越大越好，还是越小越好。比如loss是越小越好
greater_is_better= false
label_names= [tgt_input_ids]
dataloader_pin_memory= true
dataloader_num_workers= 3
fp16= true
disable_tqdm= False
report_to= [tensorboard]
save_strategy= ${evaluation_strategy} 
save_steps= ${eval_steps}
save_total_limit= 1
convert_features_in_run_time = True
warmup_steps = 2000
# lr_scheduler_type = constant_with_warmup
adam_beta2=0.9
adam_epsilon = 1e-10


[Save]
postfix = clothing
temp_dir = ${Trainer:output_dir}/temp_dir
dev_pred_file = ${Save:temp_dir}/dev_pred.json
test_pred_file = ${Save:temp_dir}/test_pred.json
best_model_file = ${Trainer:output_dir}/best_model/best.pt
log_file = ${Trainer:output_dir}/log_${postfix}.txt


[Network]
do_hp_search = False
hp_search_name = []
max_trials = 10
ada_temp = 1.5
max_gen_length = 120
num_beams = 1
do_sample = false
# ori_ada, ori_bart, ada_bart
model_type = ori_ada
dropout = 0.4
attn_dropout = 0.4

[Data]
# 按照如下顺序进行选择数据存储和输出目录
global_dir = [E:/projects/code_data/RRG, projects/RRG]
vocab_file = ${Data:global_dir}/data/vocab.pkl
# fnlp/bart-base-chinese , bert-base-chinese
pretrained_model_name_or_path = bert-base-chinese
# en , zh
language = en
train_data_repeat = -1
# 是否把验证集数据也加入训练
add_dev_data_to_train = false
max_seq_len = 390
max_train_num = -1
max_dev_num = -1








