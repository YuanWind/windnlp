[Trainer]
# The output directory where the model predictions and checkpoints will be written.
output_dir = ${Data:global_dir}/outs/${Save:postfix}
resume_from_checkpoint= None
num_train_epochs= 500
# -1代表关闭，0代表连续 4 轮没提升就停止训练，>0的数字代表具体的哪一轮停止
early_stop_mode = -1
do_train= true
do_eval= False
do_predict= False
# None, steps, epoch
evaluation_strategy= steps
eval_steps= 4500
learning_rate= 2e-05
per_device_train_batch_size= 2
gradient_accumulation_steps= 1
per_device_eval_batch_size= 2
eval_accumulation_steps= 1
seed= 42
metric_for_best_model= F1
# 决定 metric_for_best_model 是越大越好，还是越小越好。比如loss是越小越好
greater_is_better= True
label_names= [labels]
dataloader_pin_memory= true
dataloader_num_workers= 2
fp16= None
disable_tqdm= False
report_to= [tensorboard]
save_strategy= ${evaluation_strategy} 
save_steps= ${eval_steps}
save_total_limit= 1
convert_features_in_run_time = True



[Save]
postfix = debug
temp_dir = ${Trainer:output_dir}/temp_dir
best_model_file = ${Trainer:output_dir}/best_model/best.pt
log_file = ${Trainer:output_dir}/log_${postfix}.txt


[Network]
do_hp_search = False
hp_search_name = []
max_trials = 10


[Data]
# 按照如下顺序进行选择数据存储和输出目录
global_dir = [E:/projects/code_data/0_example_project, projects/0_example_project, .]
vocab_file = ${Data:global_dir}/data/vocab.pkl
train_data_repeat = -1
# 是否把验证集数据也加入训练
add_dev_data_to_train = False
pretrain_model_path = bert-base-chinese
max_seq_len = 128
max_train_num = 20
max_dev_num = 20








