[Trainer]
# The output directory where the model predictions and checkpoints will be written.
output_dir = projects/EMCGCN-ASTE/outs/${Save:postfix}
resume_from_checkpoint= None
num_train_epochs= 50
# -1代表关闭，0代表连续 4 轮没提升就停止训练，>0的数字代表具体的哪一轮停止
early_stop_mode = -1
do_train= true
do_eval= true
do_predict= False
# None, steps, epoch
evaluation_strategy= epoch
eval_steps= 100
per_device_train_batch_size= 2
gradient_accumulation_steps= 1
per_device_eval_batch_size= 2
eval_accumulation_steps= 1
seed= 42
fp16= True
# 是否使用cuda
no_cuda= false
load_best_model_at_end= True
metric_for_best_model= f_triplets_utm
# 决定 metric_for_best_model 是越大越好，还是越小越好。比如loss是越小越好
greater_is_better= True
label_names= [labels]

# ----------------------------- tricks ----------------------------------

# The label smoothing epsilon to apply (zero means no label smoothing).
label_smoothing_factor= 0.0
# [None, fgm, pgd, awp]
adversarival_type = None
adv_start_steps = 0
emb_name = emb
fgm_e= 0.3
pgd_e= 0.3
pgd_a= 0.3
pgd_k= 3
awp_a = 1.0
awp_e = 0.01
apw_k = 1
awp_param = weight
# ema, swa
other_tricks = []
ema_start_steps = 9000
ema_decay = 0.999
swa_start = 17000
swa_lr = 5e-6
swa_freq = 200
lookahead_alpha = 0.8
lookahead_K = 5
# pin_memory=true是如果报错：Warning: Leaking Caffe2 thread-pool after fork， 则改为false就没有了
dataloader_pin_memory= true
dataloader_num_workers= 8

# ----------------- Optimizer and Scheduler args -------------------------

learning_rate= 2e-05
# Weight decay for AdamW if we apply some.
weight_decay= 0.0
# Beta1 for AdamW optimizer
adam_beta1= 0.9
# Beta2 for AdamW optimizer
adam_beta2= 0.999
# Epsilon for AdamW optimizer.
adam_epsilon= 1e-08
# Max gradient norm.
max_grad_norm= 1.0
lr_scheduler_type= linear
# Linear warmup over warmup_ratio fraction of total steps.
warmup_ratio= 0.0
# Linear warmup over warmup_steps.
warmup_steps= 0

# -----------------------------logging and checkpoints args -------------------------------
# [no, steps, epoch]
logging_strategy= steps
logging_first_step= false
logging_steps= 100
logging_nan_inf_filter= true
disable_tqdm= False
report_to= [tensorboard]
# Whether or not to skip adding of memory profiler reports to metrics.
skip_memory_metrics= true
# 'adamw_hf', 'adamw_torch', 'adamw_torch_xla', 'adamw_apex_fused', 'adafactor'
optim= adamw_hf
# Whether or not to replace AdamW by Adafactor.
adafactor= false
save_strategy= ${evaluation_strategy} 
save_steps= ${eval_steps}
save_total_limit= 1
save_on_each_node= false
# If True, use gradient checkpointing to save memory at the expense of slower backward pass.
gradient_checkpointing= false
convert_features_in_run_time = true

[Save]
postfix = debug
temp_dir = ${Trainer:output_dir}/temp_dir
best_model_file = ${Trainer:output_dir}/best_model/best.pt
log_file = ${Trainer:output_dir}/log_${postfix}.txt
dev_pred_file = ${Save:temp_dir}/dev_pred_file.json
test_pred_file = ${Save:temp_dir}/test_pred_file.json
load_dataset_from_pkl = False

[Network]
do_hp_search = False
hp_search_name = []
max_trials = 10
bert_feature_dim = 768
emb_dropout = 0.5
gcn_dim = 300
num_layers = 1
# '[max, avg, sum]'
pooling = 'avg'
symmetry_decoding = False
relation_constraint = True
task = 'triplet'

[data]
vocab_file = projects/EMCGCN-ASTE/data/vocab.pkl
train_data_repeat = -1
add_dev_data_to_train = true
pretrain_model_path = bert-base-chinese
max_seq_len = 102
max_train_num = 2
max_dev_num = 20








