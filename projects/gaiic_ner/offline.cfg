[Trainer]
# The output directory where the model predictions and checkpoints will be written.
output_dir = data_gaiic_wind/output_dir/output_${Save:postfix}
resume_from_checkpoint= None
num_train_epochs= 30
# -1代表关闭，0代表连续 4 轮没提升就停止训练，>0的数字代表具体的哪一轮停止
early_stop_mode = 0
do_train= true
do_eval= true
do_predict= False
# None, steps, epoch
evaluation_strategy= epoch
eval_steps= 200
per_device_train_batch_size= 16
gradient_accumulation_steps= 1
per_device_eval_batch_size= 64
eval_accumulation_steps= 1
seed= 42
fp16= True
no_cuda= false
# simple zero_dp_2 zero_dp_3 offload auto_wrap中的一个
sharded_ddp= []
# "When using distributed training, the value of the flag `find_unused_parameters` passed to DistributedDataParallel
ddp_find_unused_parameters= None
# When using distributed training, the value of the flag `bucket_cap_mb` passed to DistributedDataParallel
ddp_bucket_cap_mb= None
# The list of keys in your dictionary of inputs that correspond to the labels.
load_best_model_at_end= True
metric_for_best_model= F1
# 决定 metric_for_best_model 是越大越好，还是越小越好。比如loss是越小越好
greater_is_better= True
label_names= [label_ids]

# ----------------------------- tricks ----------------------------------

# The label smoothing epsilon to apply (zero means no label smoothing).
label_smoothing_factor= 0.0
# [None, fgm, pgd, awp]
adversarival_type = None
adv_start_steps = 0
emb_name = emb
fgm_e= 0.3
pgd_e= 0.3
pgd_a= 0.3
pgd_k= 3
awp_a = 1.0
awp_e = 0.01
apw_k = 1
awp_param = weight
# ema, swa
other_tricks = []
ema_start_steps = 9000
ema_decay = 0.999
swa_start = 17000
swa_lr = 5e-6
swa_freq = 200
lookahead_alpha = 0.8
lookahead_K = 5
dataloader_pin_memory= true
dataloader_num_workers= 2

# ----------------- Optimizer and Scheduler args -------------------------

learning_rate= 3e-05
# Weight decay for AdamW if we apply some.
weight_decay= 1e-4
# Beta1 for AdamW optimizer
adam_beta1= 0.9
# Beta2 for AdamW optimizer
adam_beta2= 0.999
# Epsilon for AdamW optimizer.
adam_epsilon= 1e-08
# Max gradient norm.
max_grad_norm= 1.0
lr_scheduler_type= linear
# Linear warmup over warmup_ratio fraction of total steps.
warmup_ratio= 0.0
# Linear warmup over warmup_steps.
warmup_steps= 1000

# -----------------------------logging and checkpoints args -------------------------------
# [no, steps, epoch]
logging_strategy= steps
logging_first_step= false
logging_steps= 100
logging_nan_inf_filter= true
disable_tqdm= False
report_to= [tensorboard]
# Whether or not to skip adding of memory profiler reports to metrics.
skip_memory_metrics= true
# 'adamw_hf', 'adamw_torch', 'adamw_torch_xla', 'adamw_apex_fused', 'adafactor'
optim= adamw_hf
# Whether or not to replace AdamW by Adafactor.
adafactor= false
save_strategy= ${evaluation_strategy} 
save_steps= ${eval_steps}
save_total_limit= 1
save_on_each_node= false
# If True, use gradient checkpointing to save memory at the expense of slower backward pass.
gradient_checkpointing= false

[Save]
postfix = offline_1
temp_dir = ${Trainer:output_dir}/temp_dir
best_model_file = ${Trainer:output_dir}/best_model/best.pt
log_file = ${Trainer:output_dir}/log_${postfix}.txt


[Network]
do_hp_search = False
hp_search_name = []
max_trials = 10
linear_lr = 1e-4
head_size = 64
hidden_size = 768
use_gp = True
delta = -1
alpha = -1
mtl_cls_w = -1
mtl_bio_w = -1
threshold = -0.1
add_type_emb = False
type_w = 0.1
hidden_dropout_prob = 0.1
attention_probs_dropout_prob = 0.1
max_position_embeddings = 512
max_relative_position = 64

[data]
vocab_file = data_gaiic_wind/temp_data/vocab.pkl
train_data_repeat = -1
use_all_data_to_train = False
pretrained_model_name_or_path = data_gaiic_wind/nezha/nezha-cn-base
dev_pred_file = submission/dev_pred_${Save:postfix}.json
test_pred_file = submission/test_pred_${Save:postfix}.json
entity_vocab = ${Save:temp_dir}/entity_vocab.txt
max_seq_len = 128
max_train_num = -1
max_dev_num = -1







